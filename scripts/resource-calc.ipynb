{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "\n",
    "# set to True when using future work\n",
    "# v_next = False should represent what core can do today\n",
    "v_next = True\n",
    "\n",
    "# set to True when modeling validators (uses archives)\n",
    "# watchers expose endpoints for RPC that need to be modeled\n",
    "is_validator = False\n",
    "\n",
    "# set to True when comnputing limits\n",
    "# this uses settings that are \"worst case\"\n",
    "model_max = True\n",
    "\n",
    "# ledger cycle period in seconds\n",
    "ledger_time = 5\n",
    "\n",
    "# Transactions Per Ledger (derived from TPS as it's the common metric)\n",
    "if model_max:\n",
    "    classic_TPL = 5000*ledger_time\n",
    "    soroban_TPL = 2000*ledger_time\n",
    "else:\n",
    "    classic_TPL = 1000*ledger_time\n",
    "    soroban_TPL = 20*ledger_time\n",
    "\n",
    "# how long each tx type takes\n",
    "classic_tx_exec_time_ms = 0.1\n",
    "soroban_tx_exec_time_ms = 10\n",
    "\n",
    "if v_next:\n",
    "    # number of threads used during apply\n",
    "    nb_exec_lanes = 50\n",
    "    apply_time_ms = 3000\n",
    "else:\n",
    "    apply_time_ms = 1000\n",
    "\n",
    "# how long is allocated for writing\n",
    "# (may want to make this a function of classic/Soroban tps&footprints)\n",
    "if model_max:\n",
    "    apply_write_time_ms = 250\n",
    "else:\n",
    "    apply_write_time_ms = 100\n",
    "\n",
    "# tx flooding multiplier (how many unique transactions get received in steady state)\n",
    "flooding_recv_factor = 2.5\n",
    "\n",
    "# other settings\n",
    "# apply time (in addition to source account modified for seqnum & fees)\n",
    "\n",
    "if model_max:\n",
    "    #   classic\n",
    "    #      max number of LE reads per op (vnext: limited DEX)\n",
    "    classic_LE_reads_per_op = 10 if v_next else 1000\n",
    "    #      max number of LE writes per op\n",
    "    classic_LE_writes_per_op = classic_LE_reads_per_op\n",
    "    #      avg size of LE\n",
    "    classic_LE_size = 140\n",
    "\n",
    "    #   soroban\n",
    "    #      max number of LE reads\n",
    "    soroban_LE_reads_per_op = 40\n",
    "    #      max number of LE writes\n",
    "    soroban_LE_writes_per_op = 25\n",
    "    # write as much as possible, use max allowed per tx\n",
    "    soroban_LE_size = (65*1024)/soroban_LE_writes_per_op\n",
    "    # tps getledgerentries (watcher node)\n",
    "    gle_tps = 5\n",
    "else:\n",
    "    #   classic\n",
    "    #      avg number of LE reads per op \n",
    "    classic_LE_reads_per_op = 5\n",
    "    #      avg number of LE writes per op\n",
    "    classic_LE_writes_per_op = 5\n",
    "    #      avg size of LE\n",
    "    classic_LE_size = 140\n",
    "\n",
    "    #   soroban\n",
    "    #      avg number of LE reads\n",
    "    soroban_LE_reads_per_op = 10\n",
    "    #      avg number of LE writes\n",
    "    soroban_LE_writes_per_op = 5\n",
    "    # use a 10th of capacity\n",
    "    soroban_LE_size = (65*1024)/soroban_LE_writes_per_op/10\n",
    "    # tps getledgerentries (watcher node)\n",
    "    gle_tps = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket list settings\n",
    "\n",
    "# number of iops and bytes needed to read a single ledger entry\n",
    "# need to\n",
    "#    1. walk from recent to oldest buckets (up to bucket 19)\n",
    "#        bloom filter helps short circuit lookups\n",
    "#    2. for each bucket, page size on larger buckets -> more than 1 read occurs\n",
    "\n",
    "# additional extra reads (overhead)\n",
    "bl_avg_extra_nb_reads = 1\n",
    "\n",
    "if model_max:\n",
    "    # more Soroban entries in max mode\n",
    "    bl_avg_le_size = 1024\n",
    "else:\n",
    "    bl_avg_le_size = 300\n",
    "\n",
    "\n",
    "bl_avg_extra_bytes_read = bl_avg_le_size*bl_avg_extra_nb_reads\n",
    "\n",
    "bl_avg_nb_reads = 1+bl_avg_extra_nb_reads\n",
    "\n",
    "def bl_bytes_read_per_le(le_size):\n",
    "    return le_size + bl_avg_extra_bytes_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ledger_LE_reads:  50000\n",
      "ledger_bytes_read:  28775000\n",
      "ledger_LE_writes:  25000\n",
      "ledger_bytes_write:  3175000\n",
      "ledger_classic_LE_reads:  500000\n",
      "ledger_classic_bytes_read:  291000000\n",
      "ledger_classic_LE_writes:  250000\n",
      "ledger_classic_bytes_write:  35000000\n",
      "ledger_soroban_LE_reads:  800000\n",
      "ledger_soroban_bytes_read:  1474560000.0\n",
      "ledger_soroban_LE_writes:  250000\n",
      "ledger_soroban_bytes_write:  665600000.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# calculate activity per ledger (apply time)\n",
    "\n",
    "# size of an account LE\n",
    "le_account_bytes = 127\n",
    "\n",
    "# fees/seqnum\n",
    "ledger_LE_reads = classic_TPL*bl_avg_nb_reads\n",
    "ledger_bytes_read = classic_TPL*bl_bytes_read_per_le(le_account_bytes)\n",
    "ledger_LE_writes = classic_TPL\n",
    "ledger_bytes_write = ledger_LE_writes * le_account_bytes\n",
    "\n",
    "print(\"ledger_LE_reads: \", ledger_LE_reads)\n",
    "print(\"ledger_bytes_read: \", ledger_bytes_read)\n",
    "\n",
    "print(\"ledger_LE_writes: \", ledger_LE_writes)\n",
    "print(\"ledger_bytes_write: \", ledger_bytes_write)\n",
    "\n",
    "# per phase\n",
    "ledger_classic_LE_reads = classic_TPL*classic_LE_reads_per_op\n",
    "ledger_classic_bytes_read = ledger_classic_LE_reads*bl_bytes_read_per_le(classic_LE_size)\n",
    "ledger_classic_LE_reads *= bl_avg_nb_reads\n",
    "\n",
    "print(\"ledger_classic_LE_reads: \", ledger_classic_LE_reads)\n",
    "print(\"ledger_classic_bytes_read: \", ledger_classic_bytes_read)\n",
    "\n",
    "ledger_classic_LE_writes = classic_TPL*classic_LE_writes_per_op\n",
    "ledger_classic_bytes_write = ledger_classic_LE_writes*classic_LE_size\n",
    "\n",
    "print(\"ledger_classic_LE_writes: \", ledger_classic_LE_writes)\n",
    "print(\"ledger_classic_bytes_write: \", ledger_classic_bytes_write)\n",
    "\n",
    "ledger_soroban_LE_reads = soroban_TPL*soroban_LE_reads_per_op\n",
    "ledger_soroban_bytes_read = ledger_soroban_LE_reads*bl_bytes_read_per_le(soroban_LE_size)\n",
    "ledger_soroban_LE_reads *= bl_avg_nb_reads\n",
    "\n",
    "print(\"ledger_soroban_LE_reads: \", ledger_soroban_LE_reads)\n",
    "print(\"ledger_soroban_bytes_read: \", ledger_soroban_bytes_read)\n",
    "\n",
    "\n",
    "ledger_soroban_LE_writes = soroban_TPL*soroban_LE_writes_per_op\n",
    "ledger_soroban_bytes_write = ledger_soroban_LE_writes*soroban_LE_size\n",
    "\n",
    "print(\"ledger_soroban_LE_writes: \", ledger_soroban_LE_writes)\n",
    "print(\"ledger_soroban_bytes_write: \", ledger_soroban_bytes_write)\n",
    "\n",
    "\n",
    "ledger_LE_reads += ledger_classic_LE_reads + ledger_soroban_LE_reads\n",
    "ledger_bytes_read += ledger_classic_bytes_read + ledger_soroban_bytes_read\n",
    "ledger_LE_writes += ledger_classic_LE_writes + ledger_soroban_LE_writes\n",
    "ledger_bytes_write += ledger_classic_bytes_write + ledger_soroban_bytes_write\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flooding bytes (per connection):  147500000.0\n",
      "TxSet bytes:  59000000\n",
      "Network bps (GBit/s):  3.1663477420806885\n"
     ]
    }
   ],
   "source": [
    "# Overlay activity per ledger period\n",
    "\n",
    "##   bucket list\n",
    "ledger_overlay_LE_reads = ((classic_TPL+soroban_TPL)*flooding_recv_factor)\n",
    "ledger_overlay_LE_bytes = ledger_overlay_LE_reads*bl_bytes_read_per_le(le_account_bytes)\n",
    "ledger_overlay_LE_reads *= bl_avg_nb_reads\n",
    "\n",
    "##   bandwdith\n",
    "\n",
    "if v_next:\n",
    "    overlay_fan_out = 10\n",
    "else:\n",
    "    overlay_fan_out = 20\n",
    "\n",
    "if model_max:\n",
    "    tier1_size = 100\n",
    "else:\n",
    "    tier1_size = 25\n",
    "\n",
    "# how long is allocated per ledger to flooding\n",
    "ledger_overlay_processing_time_ms = ledger_time*1000\n",
    "\n",
    "classic_tx_bytes = 200\n",
    "soroban_tx_footprint = soroban_LE_reads_per_op+soroban_LE_writes_per_op\n",
    "\n",
    "# Soroban key size (as seen in footprints)\n",
    "soroban_LE_key_size_bytes = 80\n",
    "# Soroban tx size without footprint\n",
    "soroban_tx_no_footprint_size_bytes = 200\n",
    "\n",
    "soroban_tx_bytes = soroban_tx_no_footprint_size_bytes + (soroban_tx_footprint*soroban_LE_key_size_bytes)\n",
    "\n",
    "scp_message_bytes = 150\n",
    "# number of tx sets per ledger (should be 1, but due to timing issues more than 1 leader may nominate)\n",
    "ledger_scp_tx_set_count = 1.1\n",
    "\n",
    "### tx flooding\n",
    "##### use the same factor than recv\n",
    "ledger_classic_tx_flooding = (classic_TPL*flooding_recv_factor)\n",
    "ledger_soroban_tx_flooding = (soroban_TPL*flooding_recv_factor)\n",
    "ledger_tx_flood_bytes = (ledger_classic_tx_flooding*classic_tx_bytes) + (ledger_soroban_tx_flooding*soroban_tx_bytes)\n",
    "print(\"flooding bytes (per connection): \", ledger_tx_flood_bytes)\n",
    "ledger_tx_flood_bytes *= overlay_fan_out\n",
    "\n",
    "### SCP flooding\n",
    "#### 6 messages per tier1 org\n",
    "ledger_scp_messages = tier1_size*6\n",
    "ledger_scp_flood_bytes = ledger_scp_messages*scp_message_bytes\n",
    "#### tx set overhead per ledger\n",
    "ledger_txset_bytes = (classic_TPL*classic_tx_bytes) + (soroban_TPL*soroban_tx_bytes)\n",
    "print(\"TxSet bytes: \", ledger_txset_bytes)\n",
    "ledger_txset_flood_bytes = ledger_txset_bytes*ledger_scp_tx_set_count\n",
    "ledger_scp_flood_bytes += ledger_txset_flood_bytes\n",
    "\n",
    "ledger_scp_flood_bytes *= overlay_fan_out\n",
    "\n",
    "# totals\n",
    "ledger_flood_bytes = ledger_tx_flood_bytes+ledger_scp_flood_bytes\n",
    "\n",
    "flood_bps = ledger_flood_bytes / ledger_overlay_processing_time_ms * 1000\n",
    "\n",
    "print(\"Network bps (GBit/s): \", flood_bps*8/1024/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates additional \"overlay activity\" caused by watcher node activity\n",
    "if not is_validator:\n",
    "    # watchers expose the \"getledgerentries\" endpoint\n",
    "    gle_reads = ledger_time*2\n",
    "    gle_read_bytes = gle_reads*bl_bytes_read_per_le(bl_avg_le_size)\n",
    "    gle_reads *= bl_avg_nb_reads\n",
    "\n",
    "    ledger_overlay_LE_reads += gle_reads\n",
    "    ledger_overlay_LE_bytes += gle_read_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max iops:  1963575.4285714286\n",
      "max read Mbps:  2463.800726209368\n",
      "max write Mbps:  2684.6885681152344\n",
      "max disk bandwidth Mbps:  2703.9018592834473\n"
     ]
    }
   ],
   "source": [
    "# apply is equivalent to the sequence [loads, classic, Soroban, writes]\n",
    "\n",
    "if v_next:\n",
    "    # overlay can use the entire time (background apply)\n",
    "    ledger_overlay_time_ms = ledger_time*1000\n",
    "    # calculates time available to read by assuming parallel execution\n",
    "    apply_read_time_ms = apply_time_ms - apply_write_time_ms - (classic_TPL*classic_tx_exec_time_ms+soroban_TPL*soroban_tx_exec_time_ms)/nb_exec_lanes\n",
    "    assert apply_read_time_ms > 0, \"TPS too high (apply_read_time_ms <= 0)\"\n",
    "else:\n",
    "    ledger_overlay_time_ms = ledger_time*1000 - apply_time_ms\n",
    "    apply_read_time_ms = apply_time_ms - apply_write_time_ms - (classic_TPL*classic_tx_exec_time_ms+soroban_TPL*soroban_tx_exec_time_ms)\n",
    "    assert apply_read_time_ms > 0, \"TPS too high (apply_read_time_ms <= 0)\"\n",
    "\n",
    "# calculates max iops for overlay and apply separately\n",
    "\n",
    "iops_overlay = ledger_overlay_LE_reads*1000/ledger_overlay_time_ms\n",
    "iops_apply = ledger_LE_reads*1000/apply_read_time_ms\n",
    "\n",
    "rbps_overlay = ledger_overlay_LE_bytes*1000/ledger_overlay_time_ms\n",
    "\n",
    "rbps_apply = ledger_bytes_read*1000/apply_read_time_ms\n",
    "wbps_apply = ledger_bytes_write*1000/apply_write_time_ms\n",
    "\n",
    "\n",
    "\n",
    "if v_next:\n",
    "    # parallel -> add\n",
    "    max_iops = iops_overlay + iops_apply\n",
    "    max_rbps = rbps_overlay + rbps_apply\n",
    "    max_wbps = wbps_apply\n",
    "    max_bps = max(rbps_overlay + rbps_apply, rbps_overlay + wbps_apply)\n",
    "else:\n",
    "    # sequential -> max\n",
    "    max_iops = max(iops_overlay, iops_apply)\n",
    "    max_rbps = max(rbps_overlay, rbps_apply)\n",
    "    max_wbps = wbps_apply\n",
    "    max_bps = max(max_rbps, max_wbps)\n",
    "\n",
    "print(\"max iops: \", max_iops)\n",
    "print(\"max read Mbps: \", max_rbps/1024/1024)\n",
    "print(\"max write Mbps: \", max_wbps/1024/1024)\n",
    "print(\"max disk bandwidth Mbps: \", max_bps/1024/1024)\n",
    "\n",
    "# source: https://aws.amazon.com/ec2/instance-types/i3en\n",
    "max_4k_IOPS_NVMe = 2000000\n",
    "max_write_NVMe = 16*1024*1024*1024\n",
    "assert max_iops < max_4k_IOPS_NVMe, \"disk IOPS exceeded\"\n",
    "assert max_bps < (4096*max_4k_IOPS_NVMe), \"disk bandwidth exceeded\"\n",
    "assert max_wbps < max_write_NVMe, \"disk linear write bandwidth exceeded\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
